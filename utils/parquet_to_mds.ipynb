{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b71475a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from streaming.base.format.mds.writer import MDSWriter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def parquet_to_mds_modernbert(\n",
    "    parquet_file: str,\n",
    "    output_dir: str,\n",
    "    split: str,\n",
    "    chunk_size: int,\n",
    "    compression: str = None,\n",
    "    text_column: str = \"text\",\n",
    "    max_samples: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert parquet files to MDS format for ModernBERT training.\n",
    "    \n",
    "    Args:\n",
    "        parquet_file: Path to input parquet file\n",
    "        output_dir: Directory where MDS files will be saved\n",
    "        split: Split name (train/val/test)\n",
    "        chunk_size: Number of samples per batch for processing\n",
    "        compression: Compression type (None, 'snappy', 'gzip', 'brotli', 'lz4', 'zstd')\n",
    "        text_column: Name of the text column in parquet\n",
    "        max_samples: Maximum number of samples to convert (None for all)\n",
    "    \"\"\"\n",
    "    split_dir = os.path.join(output_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    \n",
    "    # Define columns as expected by ModernBERT\n",
    "    columns = {\"text\": \"str\"}\n",
    "    \n",
    "    print(f\"🔄 Converting {parquet_file} to MDS format...\")\n",
    "    print(f\"📁 Output directory: {split_dir}\")\n",
    "    print(f\"📦 Compression: {compression}\")\n",
    "    \n",
    "    with MDSWriter(columns=columns, out=split_dir, compression=compression, exist_ok=True) as writer:\n",
    "        pf = pq.ParquetFile(parquet_file)\n",
    "        total_batches = pf.metadata.num_row_groups\n",
    "        processed_samples = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pf.iter_batches(batch_size=chunk_size, columns=[text_column])):\n",
    "            texts = batch.column(text_column).to_pylist()\n",
    "            \n",
    "            for txt in texts:\n",
    "                if txt is not None and str(txt).strip():  # Skip empty/null texts\n",
    "                    writer.write({\"text\": str(txt)})\n",
    "                    processed_samples += 1\n",
    "                    \n",
    "                    if max_samples and processed_samples >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"📊 Processed {batch_idx + 1}/{total_batches} batches, {processed_samples} samples\")\n",
    "                \n",
    "            if max_samples and processed_samples >= max_samples:\n",
    "                break\n",
    "    \n",
    "    print(f\"✅ Conversion complete!\")\n",
    "    print(f\"📈 Total samples: {processed_samples}\")\n",
    "    print(f\"📂 MDS files saved to: {split_dir}\")\n",
    "    return processed_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c220275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multiple_parquets(\n",
    "    parquet_files: list,\n",
    "    output_dir: str,\n",
    "    split: str,\n",
    "    chunk_size: int,\n",
    "    compression: str = None,\n",
    "    text_column: str = \"text\"\n",
    "):\n",
    "    \"\"\"Convert multiple parquet files to a single MDS dataset.\"\"\"\n",
    "    print(f\"🚀 Converting {len(parquet_files)} parquet files to MDS...\")\n",
    "    \n",
    "    split_dir = os.path.join(output_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    columns = {\"text\": \"str\"}\n",
    "    \n",
    "    total_samples = 0\n",
    "    \n",
    "    with MDSWriter(columns=columns, out=split_dir, compression=compression, shard_size=writer_shard_size, exist_ok=True) as writer:\n",
    "        for file_idx, parquet_file in enumerate(parquet_files):\n",
    "            print(f\"\\n📄 Processing file {file_idx + 1}/{len(parquet_files)}: {parquet_file}\")\n",
    "            \n",
    "            pf = pq.ParquetFile(parquet_file)\n",
    "            file_samples = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pf.iter_batches(batch_size=chunk_size, columns=[text_column])):\n",
    "                texts = batch.column(text_column).to_pylist()\n",
    "                \n",
    "                for txt in texts:\n",
    "                    if txt is not None and str(txt).strip():\n",
    "                        writer.write({\"text\": str(txt)})\n",
    "                        file_samples += 1\n",
    "                        total_samples += 1\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"   Batch {batch_idx + 1}, samples: {file_samples}\")\n",
    "            \n",
    "            print(f\"   ✅ File complete: {file_samples} samples\")\n",
    "    \n",
    "    print(f\"\\n🎉 All files converted!\")\n",
    "    print(f\"📈 Total samples: {total_samples}\")\n",
    "    print(f\"📂 MDS dataset saved to: {split_dir}\")\n",
    "    return total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mds_dataset(mds_path: str, split: str = \"train\", sample_count: int = 5):\n",
    "    \"\"\"Validate the created MDS dataset by reading some samples.\"\"\"\n",
    "    from streaming.base.format import reader_from_json\n",
    "    import json\n",
    "    \n",
    "    split_dir = os.path.join(mds_path, split)\n",
    "    index_file = os.path.join(split_dir, \"index.json\")\n",
    "    \n",
    "    if not os.path.exists(index_file):\n",
    "        print(f\"❌ Index file not found: {index_file}\")\n",
    "        return False\n",
    "    \n",
    "    with open(index_file, 'r') as f:\n",
    "        index_data = json.load(f)\n",
    "    \n",
    "    print(f\"📋 Dataset info:\")\n",
    "    print(f\"   Shards: {len(index_data['shards'])}\")\n",
    "    print(f\"   Version: {index_data.get('version', 'unknown')}\")\n",
    "    \n",
    "    # Read some samples\n",
    "    shard_info = index_data['shards'][0]\n",
    "    shard = reader_from_json(mds_path, split, shard_info)\n",
    "    \n",
    "    print(f\"\\n📖 Sample texts:\")\n",
    "    for i in range(min(sample_count, shard.samples)):\n",
    "        sample = shard[i]\n",
    "        text = sample['text'][:100] + \"...\" if len(sample['text']) > 100 else sample['text']\n",
    "        print(f\"   Sample {i}: {text}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a08a1",
   "metadata": {},
   "source": [
    "# Convertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87289b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single parquet file conversion\n",
    "samples = parquet_to_mds_modernbert(\n",
    "    parquet_file=\"data/sentences_cleaned.parquet\",\n",
    "    output_dir=\"data/sentences_mds/\",\n",
    "    split=\"train\", # \"val\"\n",
    "    chunk_size=1_000_000,\n",
    "    compression=None,  # or \"zstd\" for compression\n",
    "    text_column=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19aa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple parquet files\n",
    "parquet_files = [\n",
    "    \"data/file1.parquet\",\n",
    "    \"data/file2.parquet\", \n",
    "    \"data/file3.parquet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_multiple_parquets(\n",
    "    parquet_files=parquet_files,\n",
    "    output_dir=\"data/combined_mds/\",\n",
    "    split=\"train\",\n",
    "    compression=\"zstd\"  # Recommended for large datasets\n",
    ")\n",
    "\n",
    "# Validate the dataset\n",
    "validate_mds_dataset(\"data/sentences_mds/\", split=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modernbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
